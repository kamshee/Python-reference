{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the world of unsupervised learning, called as such because you are not guiding, or supervising, the pattern discovery by some prediction task, but instead uncovering hidden structure from unlabeled data. Unsupervised learning encompasses a variety of techniques in machine learning, from clustering to dimension reduction to matrix factorization. In this course, you'll learn the fundamentals of unsupervised learning and implement the essential algorithms using scikit-learn and scipy. You will learn how to cluster, transform, visualize, and extract insights from unlabeled datasets, and end the course by building a recommender system to recommend popular musical artists.\n",
    "\n",
    "Outline:\n",
    "1. Clustering for dataset exploration\n",
    "2. Visualization with hierarchical clustering and t-SNE\n",
    "3. Decorrelating your data and dimension reduction - PCA\n",
    "4. Discovering interpretable features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Clustering for dataset exploration\n",
    "Discover underlying groups (or \"clusters\") in a dataset.\n",
    "\n",
    "k-means clustering\n",
    "- finds clusters of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-means clustering in sklearn\n",
    "print(samples)\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "model = KMeans(n_clusters=3)\n",
    "model.fit(samples)\n",
    "KMeans(algorithm='auto', ...)\n",
    "labels = model.predict(samples)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Cluster labels for new samples\n",
    "- new samples can be assigned to existing clusters\n",
    "- k-means remembers the mean of each cluster (the \"centroids\")\n",
    "- find the nearest centroid to each new sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster labels for new samples\n",
    "print(new_samples)\n",
    "\n",
    "new_labels = model.predict(new_samples)\n",
    "print(new_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Scatter plots to visualize\n",
    "Use Iris dataset\n",
    "- scatter plot of sepal length vs petal length\n",
    "- each point represents an iris sample\n",
    "- color points by cluster labels\n",
    "- Use PyPlot (matplotlib.pyplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot\n",
    "import matplotlib.pyplot as plt\n",
    "# sepal length is at 0\n",
    "xs = samples[:,0]\n",
    "# petal length is at 2\n",
    "ys = samples[:,2]\n",
    "plt.scatter(xs, ys, c=labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 How many clusters?\n",
    "You are given an array 'points' of size 300x2, where each row gives the (x, y) co-ordinates of a point on a map. Make a scatter plot of these points, and use the scatter plot to guess how many clusters there are.\n",
    "\n",
    "matplotlib.pyplot has already been imported as plt. In the IPython Shell:\n",
    "\n",
    "Create an array called xs that contains the values of points[:,0] - that is, column 0 of points.\n",
    "Create an array called ys that contains the values of points[:,1] - that is, column 1 of points.\n",
    "Make a scatter plot by passing xs and ys to the plt.scatter() function.\n",
    "Call the plt.show() function to show your plot.\n",
    "How many clusters do you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In [1]: xs=points[:,0]\n",
    "\n",
    "In [2]: ys=points[:,1]\n",
    "\n",
    "In [3]: plt.scatter(xs,ys)\n",
    "Out[3]: <matplotlib.collections.PathCollection at 0x7f41fd3c0d30>\n",
    "\n",
    "In [4]: plt.show()\n",
    "\n",
    "# looks like 3 clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Clustering 2D points\n",
    "From the scatter plot of the previous exercise, you saw that the points seem to separate into 3 clusters. You'll now create a KMeans model to find 3 clusters, and fit it to the data points from the previous exercise. After the model has been fit, you'll obtain the cluster labels for some new points using the .predict() method.\n",
    "\n",
    "You are given the array points from the previous exercise, and also an array new_points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import KMeans\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Create a KMeans instance with 3 clusters: model\n",
    "model = KMeans(n_clusters=3)\n",
    "\n",
    "# Fit model to points\n",
    "model.fit(points)\n",
    "\n",
    "# Determine the cluster labels of new_points: labels\n",
    "labels = model.predict(new_points)\n",
    "\n",
    "# Print cluster labels of new_points\n",
    "print(labels)\n",
    "\n",
    "# [2 1 0 2 1 2 1 1 1 0 2 1 1 0 0 1 0 0 1 1 0 1 2 1 2 0 1 0 0 2 2 1 1 1 0 2 1\n",
    "#  1 2 1 0 2 2 0 2 1 0 0 1 1 1 1 0 0 2 2 0 0 0 2 2 1 1 1 2 1 0 1 2 0 2 2 2 1\n",
    "#  2 0 0 2 1 0 2 0 2 1 0 1 0 2 1 1 1 2 1 1 2 0 0 0 0 2 1 2 0 0 2 2 1 2 0 0 2\n",
    "#  0 0 0 1 1 1 1 0 0 1 2 1 0 1 2 0 1 0 0 1 0 1 0 2 1 2 2 1 0 2 1 2 2 0 1 1 2\n",
    "#  0 2 0 1 2 0 0 2 0 1 1 0 1 0 0 1 1 2 1 1 0 2 0 2 2 1 2 1 1 2 2 0 2 2 2 0 1\n",
    "#  1 2 0 2 0 0 1 1 1 2 1 1 1 0 0 2 1 2 2 2 0 1 1 1 1 1 1 0 0 1 0 0 0 0 1 0 0\n",
    "#  1 1 2 0 2 2 0 2 0 2 0 1 1 0 1 1 1 0 2 2 0 1 1 0 1 0 0 1 0 0 2 0 2 2 2 1 0\n",
    "#  0 0 2 1 2 0 2 0 0 1 2 2 2 0 1 1 1 2 1 0 0 1 2 2 0 2 2 0 2 1 2 0 0 0 0 1 0\n",
    "#  0 1 1 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Inspect your clustering with scatter plot\n",
    "Let's now inspect the clustering you performed in the previous exercise!\n",
    "\n",
    "A solution to the previous exercise has already run, so 'new_points' is an array of points and 'labels' is the array of their cluster labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assign the columns of new_points: xs and ys\n",
    "xs = new_points[:,0]\n",
    "ys = new_points[:,1]\n",
    "\n",
    "# Make a scatter plot of xs and ys, using labels to define the colors\n",
    "plt.scatter(xs,ys,c=labels,alpha=0.5)\n",
    "\n",
    "# Assign the cluster centers: centroids\n",
    "# Compute the coordinates of the centroids \n",
    "centroids = model.cluster_centers_\n",
    "\n",
    "# Assign the columns of centroids: centroids_x, centroids_y\n",
    "centroids_x = centroids[:,0]\n",
    "centroids_y = centroids[:,1]\n",
    "\n",
    "# Make a scatter plot of centroids_x and centroids_y\n",
    "# D marker = diamonds, size of markers 50\n",
    "plt.scatter(centroids_x, centroids_y,marker='D',s=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Evaluating a clustering\n",
    "- How can you check the quality of the clustering?\n",
    "    - Can check correspondence with ie. iris species\n",
    "- If no species to check against,..\n",
    "- Measure quality of clustering\n",
    "- Informs choice of how many clusters to look for\n",
    "\n",
    "Iris: clusters vs specieis\n",
    "- k-means found 3 clusters amongst the iris samples\n",
    "- Do the clusters correspond to the species\n",
    "\n",
    "Cross tabulation with pandas\n",
    "- clusters vs species is a \"cross-tabulation\"\n",
    "- Use pandas library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a crosstab\n",
    "\n",
    "# align labels and specieis\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({'labels':labels, 'species':species})\n",
    "print(df)\n",
    "\n",
    "# crosstab of labels and species\n",
    "ct = pd.crosstab(df['labels'], df['species'])\n",
    "print(ct)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Meauring clustering quality\n",
    "- Using only samples and their cluster labels\n",
    "- a good clustering has tight clusters\n",
    "\n",
    "Inertia measures clustering quality\n",
    "- measures how spread out the clusters are (lower is better)\n",
    "- distance from each sample to centroid of its cluster\n",
    "- after fit(), avaoilable as attribute inertia_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inertia measure for clustering quality\n",
    "from sklearn.cluster import KMeans\n",
    "model = KMeans(n_clusters=3)\n",
    "model.fit(samples)\n",
    "print(model.inertia_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8 How many clusters to choose\n",
    "- a good clustering has tight clusters (so low inertia)\n",
    "- but not too many clusters\n",
    "- choose an \"elbow\" in the inertia plot\n",
    "    - elbow = where inertia begins to decrease more slowly\n",
    "    - ie. for iris datset, 3 is a good choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.9 How many clusters of grain?\n",
    "In the video, you learned how to choose a good number of clusters for a dataset using the k-means inertia graph. You are given an array samples containing the measurements (such as area, perimeter, length, and several others) of samples of grain. What's a good number of clusters in this case?\n",
    "\n",
    "KMeans and PyPlot (plt) have already been imported for you.\n",
    "\n",
    "This dataset was sourced from the UCI Machine Learning Repository.\n",
    "https://archive.ics.uci.edu/ml/datasets/seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = range(1, 6)\n",
    "inertias = []\n",
    "\n",
    "for k in ks:\n",
    "    # Create a KMeans instance with k clusters: model\n",
    "    model = KMeans(n_clusters=k)\n",
    "    \n",
    "    # Fit model to samples\n",
    "    model.fit(samples)\n",
    "    \n",
    "    # Append the inertia to the list of inertias\n",
    "    inertias.append(model.inertia_)\n",
    "    \n",
    "# Plot ks vs inertias\n",
    "plt.plot(ks, inertias, '-o')\n",
    "plt.xlabel('number of clusters, k')\n",
    "plt.ylabel('inertia')\n",
    "plt.xticks(ks)\n",
    "plt.show()\n",
    "\n",
    "# The inertia decreases very slowly from 3 clusters to 4, \n",
    "# so it looks like 3 clusters would be a good choice for this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.10 Evaluating the grain clustering\n",
    "In the previous exercise, you observed from the inertia plot that 3 is a good number of clusters for the grain data. In fact, the grain samples come from a mix of 3 different grain varieties: \"Kama\", \"Rosa\" and \"Canadian\". \n",
    "\n",
    "In this exercise, cluster the grain samples into three clusters, and compare the clusters to the grain varieties using a cross-tabulation.\n",
    "\n",
    "You have the array samples of grain samples, and a list varieties giving the grain variety for each sample. Pandas (pd) and KMeans have already been imported for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create clusters and compare in cross-tab\n",
    "\n",
    "# Create a KMeans model with 3 clusters: model\n",
    "model = KMeans(n_clusters=3)\n",
    "\n",
    "# Use fit_predict to fit model and obtain cluster labels: labels\n",
    "labels = model.fit_predict(samples)\n",
    "\n",
    "# Create a DataFrame with labels and varieties as columns: df\n",
    "df = pd.DataFrame({'labels': labels, 'varieties': varieties})\n",
    "\n",
    "# Create crosstab: ct\n",
    "ct = pd.crosstab(df['labels'],df['varieties'])\n",
    "\n",
    "# Display ct\n",
    "print(ct)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.11 Transforming features for better clusterings\n",
    "- Piedmont wines dataset\n",
    "- 178 samples from 3 distinct varieties of red wine\n",
    "- features measure chemical composition (ie. alcohol content), color intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering the wines\n",
    "from sklearn.cluster import KMeans\n",
    "model = KMeans(n_clusters=3)\n",
    "labels = model.fit_predict(samples)\n",
    "# clusters vs varieties cross-tab\n",
    "df = pd.DataFrame({'labels':labels,\n",
    "                  'varieties':varieties})\n",
    "ct = pd.crosstab(df['labels'], df['varieties'])\n",
    "print(ct)\n",
    "\n",
    "# Feature variances very different, so crosstab shoes clustering not good\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.11.a StandardScaler\n",
    "- In kmeans, feature variance = feature influence\n",
    "- the features need to be transformed to have equal variance\n",
    "- StandardScaler transforms each feature to have mean 0 and variance 1\n",
    "- Features are \"standardized\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(samples)\n",
    "StandardScaler(copy=True, with_mean=True, with_std=True)\n",
    "samples_scaled = scaler.transform(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 steps with pipeline: StandardScaler, then KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "scaler = StandardScaler()\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "pipeline = make_pipeline(scaler, kmeans)\n",
    "# fit scaler and kmeans\n",
    "pipeline.fit(samples)\n",
    "# get cluster labels\n",
    "labels = pipeline.predict(samples)\n",
    "\n",
    "# Feature standardization improves clustering\n",
    "df = pd.DataFrame({'labels':labels,\n",
    "                  'varieties':varieties})\n",
    "ct = pd.crosstab(df['labels'], df['varieties'])\n",
    "print(ct)\n",
    "\n",
    "# big improvement in clustering after standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  1.11.b sklearn preprocessing steps - few options\n",
    "- StandardScaler is a \"preprocessing\" step\n",
    "- MaxAbsScaler\n",
    "- Normalizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  1.11.c Practice: Scaling fish data for clustering\n",
    "You are given an array samples giving measurements of fish. Each row represents an individual fish. The measurements, such as weight in grams, length in centimeters, and the percentage ratio of height to length, have very different scales. In order to cluster this data effectively, you'll need to standardize these features first. In this exercise, you'll build a pipeline to standardize and cluster the data.\n",
    "\n",
    "These fish measurement data were sourced from the Journal of Statistics Education.\n",
    "http://ww2.amstat.org/publications/jse/jse_data_archive.htm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the necessary imports\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Create scaler: scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Create KMeans instance: kmeans\n",
    "kmeans = KMeans(n_clusters=4)\n",
    "\n",
    "# Create pipeline: pipeline\n",
    "pipeline = make_pipeline(scaler, kmeans)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  1.11.d Practice: Clustering the fish data\n",
    "You'll now use your standardization and clustering pipeline from the previous exercise to cluster the fish by their measurements, and then create a cross-tabulation to compare the cluster labels with the fish species.\n",
    "\n",
    "As before, samples is the 2D array of fish measurements. Your pipeline is available as pipeline, and the species of every fish sample is given by the list species.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Fit the pipeline to samples\n",
    "pipeline.fit(samples)\n",
    "\n",
    "# Calculate the cluster labels: labels\n",
    "labels = pipeline.predict(samples)\n",
    "\n",
    "# Create a DataFrame with labels and species as columns: df\n",
    "df = pd.DataFrame({'labels':labels,'species':species})\n",
    "\n",
    "# Create crosstab: ct\n",
    "ct = pd.crosstab(df['labels'], df['species'])\n",
    "\n",
    "# Display ct\n",
    "print(ct)\n",
    "\n",
    "# species  Bream  Pike  Roach  Smelt\n",
    "# labels                            \n",
    "# 0           33     0      1      0\n",
    "# 1            1     0     19      1\n",
    "# 2            0    17      0      0\n",
    "# 3            0     0      0     13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1.12 Clustering stocks using Normalizer and KMeans\n",
    "In this exercise, you'll cluster companies using their daily stock price movements (i.e. the dollar difference between the closing and opening prices for each trading day). You are given a NumPy array movements of daily price movements from 2010 to 2015 (obtained from Yahoo! Finance), where each row corresponds to a company, and each column corresponds to a trading day.\n",
    "\n",
    "Some stocks are more expensive than others. To account for this, include a Normalizer at the beginning of your pipeline. The Normalizer will separately transform each company's stock price to a relative scale before the clustering begins.\n",
    "\n",
    "Note that Normalizer() is different to StandardScaler(), which you used in the previous exercise. \n",
    "- While StandardScaler() standardizes features (such as the features of the fish data from the previous exercise) by removing the mean and scaling to unit variance, \n",
    "- Normalizer() rescales each sample - here, each company's stock price - independently of the other.\n",
    "\n",
    "KMeans and make_pipeline have already been imported for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make pipeline\n",
    "\n",
    "# Import Normalizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "# Create a normalizer: normalizer\n",
    "normalizer = Normalizer()\n",
    "\n",
    "# Create a KMeans model with 10 clusters: kmeans\n",
    "kmeans = KMeans(n_clusters=10)\n",
    "\n",
    "# Make a pipeline chaining normalizer and kmeans: pipeline\n",
    "pipeline = make_pipeline(normalizer, kmeans)\n",
    "\n",
    "# Fit pipeline to the daily price movements\n",
    "pipeline.fit(movements)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  1.12.a Which stocks move together?\n",
    "In the previous exercise, you clustered companies by their daily stock price movements. So which company have stock prices that tend to change in the same way? You'll now inspect the cluster labels from your clustering to find out.\n",
    "\n",
    "Your solution to the previous exercise has already been run. Recall that you constructed a Pipeline pipeline containing a KMeans model and fit it to the NumPy array movements of daily stock movements. In addition, a list companies of the company names is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Predict the cluster labels: labels\n",
    "labels = pipeline.predict(movements)\n",
    "\n",
    "# Create a DataFrame aligning labels and companies: df\n",
    "df = pd.DataFrame({'labels': labels, 'companies': companies})\n",
    "\n",
    "# Display df sorted by cluster label\n",
    "print(df.sort_values('labels'))\n",
    "\n",
    "                             companies  labels\n",
    "45                                Sony       0\n",
    "34                          Mitsubishi       0\n",
    "48                              Toyota       0\n",
    "7                                Canon       0\n",
    "21                               Honda       0\n",
    "32                                  3M       1\n",
    "35                            Navistar       1\n",
    "44                        Schlumberger       1\n",
    "13                   DuPont de Nemours       1\n",
    "12                             Chevron       1\n",
    "0                                Apple       1\n",
    "47                            Symantec       1\n",
    "8                          Caterpillar       1\n",
    "50  Taiwan Semiconductor Manufacturing       1\n",
    "51                   Texas instruments       1\n",
    "53                       Valero Energy       1\n",
    "57                               Exxon       1\n",
    "10                      ConocoPhillips       1\n",
    "23                                 IBM       1\n",
    "6             British American Tobacco       2\n",
    "41                       Philip Morris       2\n",
    "19                     GlaxoSmithKline       2\n",
    "36                    Northrop Grumman       3\n",
    "29                     Lookheed Martin       3\n",
    "4                               Boeing       3\n",
    "15                                Ford       4\n",
    "5                      Bank of America       4\n",
    "26                      JPMorgan Chase       4\n",
    "1                                  AIG       4\n",
    "58                               Xerox       4\n",
    "16                   General Electrics       4\n",
    "55                         Wells Fargo       4\n",
    "3                     American express       4\n",
    "18                       Goldman Sachs       4\n",
    "54                            Walgreen       5\n",
    "39                              Pfizer       5\n",
    "56                            Wal-Mart       5\n",
    "27                      Kimberly-Clark       5\n",
    "25                   Johnson & Johnson       5\n",
    "40                      Procter Gamble       5\n",
    "2                               Amazon       6\n",
    "59                               Yahoo       6\n",
    "42                   Royal Dutch Shell       7\n",
    "30                          MasterCard       7\n",
    "31                           McDonalds       7\n",
    "20                          Home Depot       7\n",
    "52                            Unilever       7\n",
    "49                               Total       7\n",
    "37                            Novartis       7\n",
    "46                      Sanofi-Aventis       7\n",
    "43                                 SAP       7\n",
    "22                                  HP       8\n",
    "17                     Google/Alphabet       8\n",
    "33                           Microsoft       8\n",
    "11                               Cisco       8\n",
    "14                                Dell       8\n",
    "24                               Intel       8\n",
    "9                    Colgate-Palmolive       9\n",
    "28                           Coca Cola       9\n",
    "38                               Pepsi       9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Visualization with hierarchical clustering and t-SNE\n",
    "2 unsupervised learning techniques for data visualization\n",
    "1. hierarchical clustering\n",
    "2. t-SNE\n",
    "\n",
    "Hierarchical clustering merges the data samples into ever-coarser clusters, yielding a tree visualization of the resulting cluster hierarchy. \n",
    "\n",
    "t-SNE maps the data samples into 2d space so that the proximity of the samples to one another can be visualized.\n",
    "- t-SNE: creates a 2D map of dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical clustering\n",
    "- Eurovision scoring dataset example\n",
    "    - countries gave scores performed at Eurovision 2006\n",
    "    - 2D array of scores\n",
    "    - rows: countries\n",
    "    - columns: songs\n",
    "- Dendrogram visualization\n",
    "    - read bottom up\n",
    "    - vertical lines represent clusters\n",
    "- Hierarchical clustering steps\n",
    "    - every country (row) begins in a separate cluster\n",
    "    - at each step, the 2 closest clusters are merged\n",
    "    - continue until all countries in a single cluster\n",
    "    - = \"agglomerative\" hierarchical clustering\n",
    "    - Divisive clustering is the opposite\n",
    "- Hierarchical clustering with SciPy\n",
    "    - given samples (the array of scores), and country_names\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hierarchical clustering with SciPy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "# linkage() performs the hierarchical clustering\n",
    "mergings = linkage(samples, method='complete')\n",
    "dendrogram(mergings,\n",
    "          labels=country_names,\n",
    "          leaf_rotation=90,\n",
    "          leaf_font_size=6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many merges?\n",
    "If there are 5 data samples, how many merge operations will occur in a hierarchical clustering? To help answer this question, think back to the video, in which Ben walked through an example of hierarchical clustering using 6 countries. How many merge operations did that example have?\n",
    "\n",
    "4 merges.\n",
    "Is it n-1?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Hierarchical clustering of the grain data\n",
    "In the video, you learned that the SciPy linkage() function performs hierarchical clustering on an array of samples. Use the linkage() function to obtain a hierarchical clustering of the grain samples, and use dendrogram() to visualize the result. A sample of the grain measurements is provided in the array samples, while the variety of each grain sample is given by the list varieties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the necessary imports\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the linkage: mergings\n",
    "mergings = linkage(samples,method='complete')\n",
    "\n",
    "# Plot the dendrogram, using varieties as labels\n",
    "dendrogram(mergings,\n",
    "           labels=varieties,\n",
    "           leaf_rotation=90,\n",
    "           leaf_font_size=6,\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Hierarchies of stocks\n",
    "In chapter 1, you used k-means clustering to cluster companies according to their stock price movements. Now, you'll perform hierarchical clustering of the companies. You are given a NumPy array of price movements movements, where the rows correspond to companies, and a list of the company names companies. SciPy hierarchical clustering doesn't fit into a sklearn pipeline, so you'll need to use the normalize() function from sklearn.preprocessing instead of Normalizer.\n",
    "\n",
    "linkage and dendrogram have already been imported from sklearn.cluster.hierarchy, and PyPlot has been imported as plt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import normalize\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# Normalize the movements: normalized_movements\n",
    "normalized_movements = normalize(movements)\n",
    "\n",
    "# Calculate the linkage: mergings\n",
    "mergings = linkage(normalized_movements,\n",
    "                    method='complete')\n",
    "\n",
    "# Plot the dendrogram\n",
    "dendrogram(mergings,\n",
    "            labels=companies,\n",
    "            leaf_rotation=90,\n",
    "            leaf_font_size=6)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Cluster labels in hierarchical clustering\n",
    "- more than just a visualization tool\n",
    "- cluster labels recovered at any intermediate stage\n",
    "- intermediate cluster labels can be used in cross tabulations\n",
    "\n",
    "Intermediate clusterings and height on dendrogram\n",
    "- ie. at height 15: Bulgaria, Cyprus, Greece are one cluster\n",
    "- Height on dendrogram = distance b/n merging clusters\n",
    "- Height on dendrogram specifies max distance b/n merging clusters\n",
    "    - Don't merge clusters further apart than this (ie. 15)\n",
    "\n",
    "Distance b/n clusters\n",
    "- defined by a \"linkage method\"\n",
    "- specified via method parameter, ie. linkage(samples,method='complete)\n",
    "- in \"complete\" linkage: distance b/n clusters is max distance b/n their samples\n",
    "- different linkage methods yield different hierarchical clustering\n",
    "\n",
    "Extracting cluster labels\n",
    "- use fcluster method\n",
    "- returns a NumPy array of cluster labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.a Extracting cluster labels using fcluster "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage\n",
    "mergings = linkage(samples, method='complete')\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "labels = fcluster(mergings, 15, criterion='distance')\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.b Aligning cluster labels with country names\n",
    "- give a list of string: country_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pairs = pd.DataFrame({'labels':labels,\n",
    "                      'countries':country_names})\n",
    "# sort by cluster label and print\n",
    "print(pairs.sort_values('labels'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Which clusters are closest?\n",
    "In the video, you learned that the linkage method defines how the distance between clusters is measured. In complete linkage, the distance between clusters is the distance between the furthest points of the clusters. In single linkage, the distance between clusters is the distance between the closest points of the clusters.\n",
    "\n",
    "Consider the three clusters in the diagram. Which of the following statements are true?\n",
    "\n",
    "A. In single linkage, cluster 3 is the closest to cluster 2.\n",
    "\n",
    "B. In complete linkage, cluster 1 is the closest to cluster 2.\n",
    "\n",
    "Answer: both - but need plot to answer this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Different linkage, different hierarchical clustering!\n",
    "In the video, you saw a hierarchical clustering of the voting countries at the Eurovision song contest using 'complete' linkage. Now, perform a hierarchical clustering of the voting countries with 'single' linkage, and compare the resulting dendrogram with the one in the video. Different linkage, different hierarchical clustering!\n",
    "\n",
    "You are given an array samples. Each row corresponds to a voting country, and each column corresponds to a performance that was voted for. \n",
    "\n",
    "The list country_names gives the name of each voting country. This dataset was obtained from Eurovision.\n",
    "https://eurovision.tv/history/full-split-results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method:\"single\" for single linkage\n",
    "\n",
    "# Perform the necessary imports\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "\n",
    "# Calculate the linkage: mergings\n",
    "mergings = linkage(samples,method='single')\n",
    "\n",
    "# Plot the dendrogram\n",
    "dendrogram(mergings,\n",
    "            labels=country_names,\n",
    "            leaf_rotation=90,\n",
    "            leaf_font_size=6)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Intermediate clusterings\n",
    "Displayed on the right is the dendrogram for the hierarchical clustering of the grain samples that you computed earlier. If the hierarchical clustering were stopped at height 6 on the dendrogram, how many clusters would there be?\n",
    "answer: 3 - need to see plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Extracting the cluster labels\n",
    "In the previous exercise, you saw that the intermediate clustering of the grain samples at height 6 has 3 clusters. Now, use the fcluster() function to extract the cluster labels for this intermediate clustering, and compare the labels with the grain varieties using a cross-tabulation.\n",
    "\n",
    "The hierarchical clustering has already been performed and mergings is the result of the linkage() function. The list varieties gives the variety of each grain sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the necessary imports\n",
    "import pandas as pd\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "\n",
    "# Use fcluster to extract labels: labels\n",
    "labels = fcluster(mergings,6,criterion='distance')\n",
    "\n",
    "# Create a DataFrame with labels and varieties as columns: df\n",
    "df = pd.DataFrame({'labels': labels, 'varieties': varieties})\n",
    "\n",
    "# Create crosstab: ct\n",
    "ct = pd.crosstab(df['labels'],df['varieties'])\n",
    "\n",
    "# Display ct\n",
    "print(ct)\n",
    "\n",
    "# varieties  Canadian wheat  Kama wheat  Rosa wheat\n",
    "# labels                                           \n",
    "# 1                      14           3           0\n",
    "# 2                       0           0          14\n",
    "# 3                       0          11           0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 t-SNE for 2-dimensional maps\n",
    "- t-SNE = t-distributted stochastic neighbor embedding\n",
    "- maps samples from high-dimensional space to 2D space (or 3D)\n",
    "- great job appoximately preserves nearness of samples\n",
    "- great for inspecting datasets\n",
    "\n",
    "Iris dataset has 4 measurements, so samples are 4-dimensional\n",
    "- t-SNE maps samples to 2D space\n",
    "- t-SNE didn't know there were different species\n",
    "    - yet kept the species mostly separate\n",
    "- we learn 2 species (versicolor and virginica) have samples close together in space\n",
    "    - consistent with k-means inertia plot (tight clusters): could argue for 2 clusters, or for 3\n",
    "    \n",
    "t-SNE in sklearn\n",
    "- print(samples)\n",
    "    - 2D NumPy array samples\n",
    "    - List species giving species of labels as numbers (0,1, or 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE in sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "model = TSNE(learning_rate=100)\n",
    "transformed = model.fit_transform(samples)\n",
    "xs = transformed[:,0]\n",
    "ys = transformed[:,1]\n",
    "plt.scatter(xs, ys, c=species)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### t-SNE has only fit_transform()\n",
    "- has a fit_transform() method\n",
    "- simulaneously fits the model and transforms the data\n",
    "- has no separate fit() or transform() methods\n",
    "    - so, can not extend the map to include NEW data samples\n",
    "    - must start over each time\n",
    "\n",
    "#### t-SNE learning rate\n",
    "- choose learning rate for the dataset\n",
    "- if wrong choice: points bunch together\n",
    "- advice: normally enough to try values b/n 50 and 200\n",
    "\n",
    "#### Different every time\n",
    "- t-SNE features are different every time\n",
    "- the axis of the plot do not have any interpretable meaning\n",
    "- example: although Piedmont wines data, 3 runs, yield 3 different scatter plots\n",
    "    - however, the wine varieties (=colors) have same position relative to one another\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.8.a t-SNE visualization of grain dataset\n",
    "In the video, you saw t-SNE applied to the iris dataset. In this exercise, you'll apply t-SNE to the grain samples data and inspect the resulting t-SNE features using a scatter plot. You are given an array samples of grain samples and a list variety_numbers giving the variety number of each grain sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TSNE\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Create a TSNE instance: model\n",
    "model = TSNE(learning_rate=200)\n",
    "\n",
    "# Apply fit_transform to samples: tsne_features\n",
    "tsne_features = model.fit_transform(samples)\n",
    "\n",
    "# Select the 0th feature: xs\n",
    "xs = tsne_features[:,0]\n",
    "\n",
    "# Select the 1st feature: ys\n",
    "ys = tsne_features[:,1]\n",
    "\n",
    "# Scatter plot, coloring by variety_numbers\n",
    "plt.scatter(xs, ys, c=variety_numbers)\n",
    "plt.show()\n",
    "\n",
    "# the t-SNE visualization manages to separate the 3 varieties\n",
    "# of grain samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.8.b A t-SNE map of the stock market\n",
    "t-SNE provides great visualizations when the individual samples can be labeled. In this exercise, you'll apply t-SNE to the company stock price data. A scatter plot of the resulting t-SNE features, labeled by the company names, gives you a map of the stock market! The stock price movements for each company are available as the array normalized_movements (these have already been normalized for you). The list companies gives the name of each company. PyPlot (plt) has been imported for you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TSNE\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Create a TSNE instance: model\n",
    "model = TSNE(learning_rate=50)\n",
    "\n",
    "# Apply fit_transform to normalized_movements: tsne_features\n",
    "tsne_features = model.fit_transform(normalized_movements)\n",
    "\n",
    "# Select the 0th feature: xs\n",
    "xs = tsne_features[:,0]\n",
    "\n",
    "# Select the 1th feature: ys\n",
    "ys = tsne_features[:,1]\n",
    "\n",
    "# Scatter plot\n",
    "plt.scatter(xs, ys, alpha=0.5)\n",
    "\n",
    "# Annotate the points\n",
    "for x, y, company in zip(xs, ys, companies):\n",
    "    plt.annotate(company, (x, y), fontsize=5, alpha=0.75)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Decorrelating your data and dimension reduction - PCA\n",
    "Dimension reduction summarizes a dataset using its common occuring patterns.\n",
    "\n",
    "Learn about the most fundamental of dimension reduction techniques, \"Principal Component Analysis\" (\"PCA\"). PCA is often used before supervised learning to improve model performance and generalization. It can also be useful for unsupervised learning. For example, you'll employ a variant of PCA will allow you to cluster Wikipedia articles by their content!\n",
    "\n",
    "## Dimension reduction\n",
    "- more efficient storage and computation\n",
    "- most important function: remove less-information \"noise\" features\n",
    "    - those \"noise\" features cause problems for prediction tasks like classification and regression\n",
    "\n",
    "## Principal Component Analysis\n",
    "- PCA = \"Principal Component Analysis\"\n",
    "- Fundamental dimension reduction technique\n",
    "- 2 steps\n",
    "    - first step: \"decorrelation\" \n",
    "    - second step: reduces dimension\n",
    "- Decorrelation:\n",
    "    - = PCA rotates data samples to be aligned with axis\n",
    "    - shifts data samples to have mean=0\n",
    "    - No information is lost\n",
    "    \n",
    "## PCA follows the fit/transform pattern\n",
    "- PCA a scikit-learn component like KMeans or StandardScaler\n",
    "- fit() learns the transformation from given ata\n",
    "- transform() applies the learned transformation\n",
    "- transform() can also be applied to NEW unseen data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Using sklearn PCA\n",
    "- samples = array of two wine features (total_phenols & o228o\n",
    "\n",
    "## PCA features\n",
    "- Rows of transformed correspond to samples\n",
    "- Columns of transformed are the \"PCA features\"\n",
    "- Row gives PCA feature values of corresponding sample\n",
    "\n",
    "## PCA features are not correlated - due to rotation performed\n",
    "- resulting PCA features are not linearly correlated (\"decorrelation\")\n",
    "\n",
    "## Pearson correlation\n",
    "- measures linear correlation of features\n",
    "- value b/n -1 and 1\n",
    "    - larger values indicate stronger correlation\n",
    "- value of 0 means NO linear correlation\n",
    "\n",
    "## Principal components\n",
    "- \"Principal components\" = directions of variance\n",
    "- PCA aligns principal components with the axes\n",
    "- available as components_ attribute of PCA object\n",
    "- each row defines displacement from mean\n",
    "    - numpy array with 1 row for each principal component\n",
    "    - print(model.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "print(samples)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "model = PCA()\n",
    "model.fit(samples)\n",
    "# new array of transformed samples\n",
    "transformed = model.transform(samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Correlated data in nature\n",
    "You are given an array grains giving the width and length of samples of grain. You suspect that width and length will be correlated. To confirm this, make a scatter plot of width vs length and measure their Pearson correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the necessary imports\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Assign the 0th column of grains: width\n",
    "width = grains[:,0]\n",
    "\n",
    "# Assign the 1st column of grains: length\n",
    "length = grains[:,1]\n",
    "\n",
    "# Scatter plot width vs length\n",
    "plt.scatter(width, length)\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "\n",
    "# Calculate the Pearson correlation\n",
    "correlation, pvalue = pearsonr(width,length)\n",
    "\n",
    "# Display the correlation\n",
    "print(correlation)\n",
    "\n",
    "# 0.860414937714\n",
    "# which is highly correlated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Decorrelating the grain measurements with PCA\n",
    "- You observed in the previous exercise that the width and length measurements of the grain are correlated. \n",
    "- Now, use PCA to decorrelate these measurements,\n",
    "- then plot the decorrelated points and measure their Pearson correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create PCA instance: model\n",
    "model = PCA()\n",
    "\n",
    "# Apply the fit_transform method of model to grains: pca_features\n",
    "pca_features = model.fit_transform(grains)\n",
    "\n",
    "# Assign 0th column of pca_features: xs\n",
    "xs = pca_features[:,0]\n",
    "\n",
    "# Assign 1st column of pca_features: ys\n",
    "ys = pca_features[:,1]\n",
    "\n",
    "# Scatter plot xs vs ys\n",
    "plt.scatter(xs, ys)\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "\n",
    "# Calculate the Pearson correlation of xs and ys\n",
    "correlation, pvalue = pearsonr(xs, ys)\n",
    "\n",
    "# Display the correlation\n",
    "print(correlation)\n",
    "\n",
    "# 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Intrinsic dimension\n",
    "Intrinsic dimension of a flight path\n",
    "- consider 2 features: longitude and latitude at points along a flight path\n",
    "- but can be appoximated using 1 feature: displacement along flight path\n",
    "- data is intrinsically 1 dimensional\n",
    "\n",
    "Intrinsic dimension = number of features needed to approximate the dataset\n",
    "- essential idea behind dimension reduction\n",
    "- What is the most compact representation of the samples?\n",
    "- Can be detected with PCA\n",
    "\n",
    "Example from Versicolor dataset\n",
    "- versicolor is one of the iris species\n",
    "- only 3 features: sepal length, sepal width, and petal width\n",
    "- samples are points in 3D space\n",
    "- if you make a 3D scatterplot, samples lie close to a flat 2D sheet\n",
    "    - so can be approximated using 2 features\n",
    "    - intrinsic dimension = 2\n",
    "    \n",
    "PCA identifies intrinsic dimension\n",
    "- scatter plots work only if samples have 2 or 3 features\n",
    "- PCA identifies intrinsic dimension when samples have any number of features\n",
    "- Intrinsic dimension = number of PCA features with significant variance\n",
    "- PCA applied to versicolor samples\n",
    "    - PCA rotates and shifts the samples to align with the coordinate axis\n",
    "    - 3 features expressed\n",
    "    - use bar graph to see variance of the 3 features\n",
    "\n",
    "Variance and intrinsic dimension\n",
    "- intrinsic dimension is number of PCA features with signficant variance\n",
    "- for versicolor example, only first 2 features (of 3) have signficant variance\n",
    "    - so, intrinsic dimension = 2\n",
    "    - this agrees with scatter plot observation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.a Plotting the variances of PCA features\n",
    "- samples = array of versicolor samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "pca.fit(samples)\n",
    "features = range(pca.n_components_)\n",
    "\n",
    "# make bar plot of variances\n",
    "plt.bar(features, pca.explained_variance_)\n",
    "plt.xticks(features)\n",
    "plt.ylabel('variance')\n",
    "plt.xlabel('PCA feature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.b Intrinsic dimension can be ambiguous\n",
    "- intrinsic dimension is an idealization\n",
    "- ...there is not always one correct answer\n",
    "    - depends on the threshold you choose\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.c The first principal component - find and draw arrow on scatter plot\n",
    "The first principal component of the data is the direction in which the data varies the most. \n",
    "\n",
    "In this exercise, your job is to \n",
    "- use PCA to find the first principal component of the length and width measurements of the grain samples, and \n",
    "- represent it as an arrow on the scatter plot.\n",
    "\n",
    "The array grains gives the length and width of the grain samples. PyPlot (plt) and PCA have already been imported for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a scatter plot of the untransformed points\n",
    "plt.scatter(grains[:,0], grains[:,1])\n",
    "\n",
    "# Create a PCA instance: model\n",
    "model = PCA()\n",
    "\n",
    "# Fit model to points\n",
    "model.fit(grains)\n",
    "\n",
    "# Get the mean of the grain samples: mean\n",
    "mean = model.mean_\n",
    "\n",
    "# Get the first principal component: first_pc\n",
    "first_pc = model.components_[0,:]\n",
    "\n",
    "# Plot first_pc as an arrow, starting at mean\n",
    "plt.arrow(mean[0], mean[1], first_pc[0], first_pc[1], color='red', width=0.01)\n",
    "\n",
    "# Keep axes on same scale\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.d  Variance of the PCA features\n",
    "The fish dataset is 6-dimensional. But what is its intrinsic dimension? \n",
    "- Make a plot of the variances of the PCA features to find out. \n",
    "- As before, samples is a 2D array, where each row represents a fish. \n",
    "- You'll need to standardize the features first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the necessary imports\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create scaler: scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Create a PCA instance: pca\n",
    "pca = PCA()\n",
    "\n",
    "# Create pipeline: pipeline\n",
    "pipeline = make_pipeline(scaler, pca)\n",
    "\n",
    "# Fit the pipeline to 'samples'\n",
    "pipeline.fit(samples)\n",
    "\n",
    "# Plot the explained variances\n",
    "features = range(pca.n_components_)\n",
    "plt.bar(features, pca.explained_variance_)\n",
    "plt.xlabel('PCA feature')\n",
    "plt.ylabel('variance')\n",
    "plt.xticks(features)\n",
    "plt.show()\n",
    "\n",
    "# PCA features 0 and 1 have significant variance.\n",
    "# intrinsic dimension = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Dimension Reduction with PCA\n",
    "Dimension reduction\n",
    "- represents same data, using less features\n",
    "- important part of machine-learning pipelines\n",
    "- PCA features are in decresing order of variance\n",
    "    - assumes low variance features are \"noise\"\n",
    "    - ...and high variance features are informative\n",
    "- specify how many features to keep\n",
    "    - ie. PCA(n_components=2)\n",
    "        - keeps the first 2 PCA features\n",
    "    - intrinsic dimension is a good choice\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.a Example: dimension reduction of iris dataset\n",
    "- samples=array of iris measurements (4 features)\n",
    "- species=list of iris species numbers\n",
    "- Try to reduce to 2 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA on iris dataset\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(samples)\n",
    "transformed = pca.transform(samples)\n",
    "print(transformed.shape)\n",
    "# (150, 2)\n",
    "# you see 2 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iris dataset in 2 dimensions\n",
    "# PCA reduced the dimension to 2\n",
    "# Retained the 2 PCA features with highest variance\n",
    "# These 2 features are very informative as important info preserved\n",
    "#  since the 3 species remain distinct\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "xs = transformed[:,0]\n",
    "ys = transformed[:,1]\n",
    "\n",
    "plt.scatter(xs, ys, c=species)\n",
    "plt.show()\n",
    "\n",
    "# with 2 features, you can still 3 species in the scatter plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.b Dimension reduction with PCA...\n",
    "- discards low variance PCA features\n",
    "- assumes the high variance features are informative\n",
    "- Assumption typically holds in practice\n",
    "\n",
    "#### Some cases where this does NOT hold, so use an alternative form of PCA\n",
    "- Word frequency arrays (\"tf-idf\") - great example\n",
    "    - what is it? each row a document, each column a word from a fixed vocabulary\n",
    "    - this is a sparse array example\n",
    "- Sparse arrays and csr_matrix\n",
    "    - sparse arrays: most entries are 0\n",
    "    - Can use scipy.sparse.csr_matrix instead of NumPy array\n",
    "    - csr_matrix remembers only non-zero entries (and saves space)\n",
    "    \n",
    "#### TruncatedSVD and csr_matrix\n",
    "- scikit-learn PCA doesn't support csr_matrix\n",
    "- use TruncatedSVD instead in sklearn\n",
    "- performs same transformation as PCA, but able to accept csr_matrix as input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.c TruncatedSVD and csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "model = TruncatedSVD(n_components=3)\n",
    "# documents is a csr_matrix\n",
    "model.fit(documents)\n",
    "transformed = model.transform(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.d Dimension reduction of the fish measurements\n",
    "In a previous exercise, you saw that 2 was a reasonable choice for the \"intrinsic dimension\" of the fish measurements. Now use PCA for dimensionality reduction of the fish measurements, retaining only the 2 most important components.\n",
    "\n",
    "The fish measurements have already been scaled for you, and are available as scaled_samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create a PCA model with 2 components: pca\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Fit the PCA instance to the scaled samples\n",
    "pca.fit(scaled_samples)\n",
    "\n",
    "# Transform the scaled samples: pca_features\n",
    "pca_features = pca.transform(scaled_samples)\n",
    "\n",
    "# Print the shape of pca_features\n",
    "print(pca_features.shape)\n",
    "\n",
    "# (85, 2)\n",
    "# reduced dimensionality from 6 to 2 (see columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.e A tf-idf word-frequency array\n",
    "In this exercise, you'll create a tf-idf word frequency array for a toy collection of documents. \n",
    "- use the TfidfVectorizer from sklearn. \n",
    "    - It transforms a list of documents into a word frequency array, which it outputs as a csr_matrix. \n",
    "    - It has fit() and transform() methods like other sklearn objects.\n",
    "\n",
    "You are given a list 'documents' of toy documents about pets.\n",
    "- ['cats say meow', 'dogs say woof', 'dogs chase cats']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create a TfidfVectorizer: tfidf\n",
    "tfidf = TfidfVectorizer() \n",
    "\n",
    "# Apply fit_transform to document: csr_mat\n",
    "csr_mat = tfidf.fit_transform(documents)\n",
    "\n",
    "# Print result of toarray() method\n",
    "print(csr_mat.toarray())\n",
    "\n",
    "# Get the words: words\n",
    "words = tfidf.get_feature_names()\n",
    "\n",
    "# Print words\n",
    "print(words)\n",
    "\n",
    "# [[ 0.51785612  0.          0.          0.68091856  0.51785612  0.        ]\n",
    "#  [ 0.          0.          0.51785612  0.          0.51785612  0.68091856]\n",
    "#  [ 0.51785612  0.68091856  0.51785612  0.          0.          0.        ]]\n",
    "# ['cats', 'chase', 'dogs', 'meow', 'say', 'woof']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.f Clustering Wikipedia part I\n",
    "You saw in the video that TruncatedSVD is able to perform PCA on sparse arrays in csr_matrix format, such as word-frequency arrays. \n",
    "\n",
    "Combine your knowledge of TruncatedSVD and k-means to cluster some popular pages from Wikipedia. \n",
    "\n",
    "In this exercise, build the pipeline. In the next exercise, you'll apply it to the word-frequency array of some Wikipedia articles.\n",
    "\n",
    "Create a Pipeline object consisting of a TruncatedSVD followed by KMeans. (This time, we've precomputed the word-frequency matrix for you, so there's no need for a TfidfVectorizer).\n",
    "\n",
    "The Wikipedia dataset you will be working with was obtained from here.\n",
    "https://blog.lateral.io/2015/06/the-unknown-perils-of-mining-wikipedia/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup pipeline for TruncatedSVD\n",
    "\n",
    "# Perform the necessary imports\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Create a TruncatedSVD instance: svd\n",
    "svd = TruncatedSVD(n_components=50)\n",
    "\n",
    "# Create a KMeans instance: kmeans\n",
    "kmeans = KMeans(n_clusters=6)\n",
    "\n",
    "# Create a pipeline: pipeline\n",
    "pipeline = make_pipeline(svd, kmeans)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.g Clustering Wikipedia part II\n",
    "It is now time to put your pipeline from the previous exercise to work! You are given an array articles of tf-idf word-frequencies of some popular Wikipedia articles, and a list titles of their titles. Use your pipeline to cluster the Wikipedia articles.\n",
    "\n",
    "A solution to the previous exercise has been pre-loaded for you, so a Pipeline 'pipeline' chaining TruncatedSVD with KMeans is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pipeline above to cluster \n",
    "\n",
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Fit the pipeline to articles\n",
    "pipeline.fit(articles)\n",
    "\n",
    "# Calculate the cluster labels: labels\n",
    "labels = pipeline.predict(articles)\n",
    "\n",
    "# Create a DataFrame aligning labels and titles: df\n",
    "df = pd.DataFrame({'label': labels, 'article': titles})\n",
    "\n",
    "# Display df sorted by cluster label\n",
    "print(df.sort_values('label'))\n",
    "\n",
    "#                                           article  label\n",
    "# 29                               Jennifer Aniston      0\n",
    "# 22                              Denzel Washington      0\n",
    "# 23                           Catherine Zeta-Jones      0\n",
    "# 24                                   Jessica Biel      0\n",
    "# 25                                  Russell Crowe      0\n",
    "# 26                                     Mila Kunis      0\n",
    "# 27                                 Dakota Fanning      0\n",
    "# 28                                  Anne Hathaway      0\n",
    "# 21                             Michael Fassbender      0\n",
    "# 20                                 Angelina Jolie      0\n",
    "# 40                                    Tonsillitis      1\n",
    "# 43                                       Leukemia      1\n",
    "# 44                                           Gout      1\n",
    "# 45                                    Hepatitis C      1\n",
    "# 46                                     Prednisone      1\n",
    "# 47                                          Fever      1\n",
    "# 48                                     Gabapentin      1\n",
    "# 49                                       Lymphoma      1\n",
    "# 42                                    Doxycycline      1\n",
    "# 41                                    Hepatitis B      1\n",
    "# 0                                        HTTP 404      1\n",
    "# 1                                  Alexa Internet      1\n",
    "# 2                               Internet Explorer      1\n",
    "# 3                                     HTTP cookie      1\n",
    "# 4                                   Google Search      1\n",
    "# 5                                          Tumblr      1\n",
    "# 6                     Hypertext Transfer Protocol      1\n",
    "# 7                                   Social search      1\n",
    "# 8                                         Firefox      1\n",
    "# 9                                        LinkedIn      1\n",
    "# 18  2010 United Nations Climate Change Conference      2\n",
    "# 19  2007 United Nations Climate Change Conference      2\n",
    "# 10                                 Global warming      2\n",
    "# 14                                 Climate change      2\n",
    "# 15                                 Kyoto Protocol      2\n",
    "# 13                               Connie Hedegaard      2\n",
    "# 12                                   Nigel Lawson      2\n",
    "# 11       Nationally Appropriate Mitigation Action      2\n",
    "# 16                                        350.org      2\n",
    "# 17  Greenhouse gas emissions by the United States      2\n",
    "# 36              2014 FIFA World Cup qualification      3\n",
    "# 35                Colombia national football team      3\n",
    "# 30                  France national football team      3\n",
    "# 53                                   Stevie Nicks      4\n",
    "# 55                                  Black Sabbath      4\n",
    "# 52                                     The Wanted      4\n",
    "# 56                                       Skrillex      4\n",
    "# 51                                     Nate Ruess      4\n",
    "# 57                          Red Hot Chili Peppers      4\n",
    "# 54                                 Arctic Monkeys      4\n",
    "# 59                                    Adam Levine      4\n",
    "# 58                                         Sepsis      4\n",
    "# 50                                   Chad Kroeger      4\n",
    "# 32                                   Arsenal F.C.      5\n",
    "# 31                              Cristiano Ronaldo      5\n",
    "# 39                                  Franck Ribry      5\n",
    "# 38                                         Neymar      5\n",
    "# 37                                       Football      5\n",
    "# 34                             Zlatan Ibrahimovi      5\n",
    "# 33                                 Radamel Falcao      5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. NMF - Discovering interpretable features\n",
    "NMF = non-negative matrix factorization\n",
    "- Dimension reduction technique that expresses samples as combinations of interpretable parts. \n",
    "- NMF models are INTERPRETABLE (unlike PCA)\n",
    "    - easy to interpret and easy to explian\n",
    "- However, all sample features must be non-negative (>=0)\n",
    "\n",
    "Interpretable parts\n",
    "- interpretable by decomposing samples as sums of their parts\n",
    "- examples\n",
    "    - it expresses documents as combinations of topics\n",
    "    - expresses images in terms of combos of visual patterns\n",
    "    \n",
    "- You'll also learn to use NMF to build recommender systems that can find you similar articles to read, or musical artists that match your listening history!\n",
    "\n",
    "## Using scikit-learn NMF\n",
    "- follows fit() and transform() pattern\n",
    "- BUT must specify number of components\n",
    "    - NMF(n_components=2)\n",
    "- works with both NumPy arrays and with csr_matrix\n",
    "\n",
    "## Example word-frequency array\n",
    "- word frequency array, 4 words, many documents\n",
    "- measure presence of words in each document using \"tf-idf\"\n",
    "    - recall tf-idf measures word frequency\n",
    "- \"tf\" = frequency of word in document\n",
    "    - if 10% of words is \"DataCamp\" then value = 0.1\n",
    "- \"idf\" = reduces influence of frequent words (ie. the)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Example using NMF\n",
    "- samples is the word-frequency array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "# remember you need to specify number of components for NMF\n",
    "model = NMF(n_components=2)\n",
    "model.fit(samples)\n",
    "nmf_features = model.transform(samples)\n",
    "\n",
    "print(model.components_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 NMF components: model.components_\n",
    "- NMF has components (like PCA)\n",
    "- Dimension of components = dimension of samples\n",
    "    - above example has 2 components in a 4-dimensional space (4 words)\n",
    "- Entries are non-negative\n",
    "\n",
    "### 4.3 NMF features: nmf_features\n",
    "- NMF feature values are non-negative\n",
    "    - following above example, 2 features columns due to 2 components\n",
    "- Features can be used to reconstruct the samples\n",
    "    - ...combine feature values with components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Reconstruction of a samples\n",
    "print(samples[i,:])\n",
    "\n",
    "print(nmf_features[i,:])\n",
    "\n",
    "#### Sample reconstruction\n",
    "- multiply components by features values, and add up\n",
    "- can also be expressed as a product of matrices\n",
    "    - that' why it's called \"matrix factorization\"\n",
    "\n",
    "#### NMF fits to non-negative data, only\n",
    "- word frequencies in each document\n",
    "- images endoded as arrays\n",
    "- audio spectograms\n",
    "- purchase histories on e-commerce sites\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 NMF applied to Wikipedia articles\n",
    "In the video, you saw NMF applied to transform a toy word-frequency array. Now it's your turn to apply NMF, this time using the tf-idf word-frequency array of Wikipedia articles, given as a csr matrix articles. Here, fit the model and transform the articles. In the next exercise, you'll explore the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import NMF\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# Create an NMF instance: model\n",
    "model = NMF(n_components=6)\n",
    "\n",
    "# Fit the model to articles, which is the word count data\n",
    "model.fit(articles)\n",
    "\n",
    "# Transform the articles: nmf_features\n",
    "nmf_features = model.transform(articles)\n",
    "\n",
    "# Print the NMF features\n",
    "print(nmf_features)\n",
    "\n",
    "# [[  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
    "#     0.00000000e+00   4.40506373e-01]\n",
    "#  [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
    "#     0.00000000e+00   5.66659188e-01]\n",
    "#  [  3.82050268e-03   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
    "#     0.00000000e+00   3.98683767e-01]\n",
    "#  [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
    "#     0.00000000e+00   3.81775228e-01]\n",
    "#  [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
    "#     0.00000000e+00   4.85564507e-01]\n",
    "#  [  1.29287389e-02   1.37901036e-02   7.76210545e-03   3.34474992e-02\n",
    "#     0.00000000e+00   3.34554761e-01]\n",
    "#  [  0.00000000e+00   0.00000000e+00   2.06716073e-02   0.00000000e+00\n",
    "#     6.04727411e-03   3.59094015e-01]\n",
    "#  [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
    "#     0.00000000e+00   4.91024756e-01]\n",
    "#  [  1.54269946e-02   1.42828850e-02   3.76586104e-03   2.37106658e-02\n",
    "#     2.62721416e-02   4.80819557e-01]\n",
    "#  [  1.11735727e-02   3.13703233e-02   3.09443015e-02   6.56980805e-02\n",
    "#     1.96750899e-02   3.38321972e-01]\n",
    "#  [  0.00000000e+00   0.00000000e+00   5.30647953e-01   0.00000000e+00\n",
    "#     2.83788404e-02   0.00000000e+00]\n",
    "#  [  0.00000000e+00   0.00000000e+00   3.56461373e-01   0.00000000e+00\n",
    "#     0.00000000e+00   0.00000000e+00]\n",
    "#  [  1.20124018e-02   6.50088586e-03   3.12203197e-01   6.09756724e-02\n",
    "#     1.13905116e-02   1.92621126e-02]\n",
    "#  [  3.93475069e-03   6.24484146e-03   3.42327174e-01   1.10766620e-02\n",
    "#     0.00000000e+00   0.00000000e+00]\n",
    "#  [  4.63808495e-03   0.00000000e+00   4.34856437e-01   0.00000000e+00\n",
    "#     3.84422147e-02   3.08165696e-03]\n",
    "#  [  0.00000000e+00   0.00000000e+00   4.83224134e-01   0.00000000e+00\n",
    "#     0.00000000e+00   0.00000000e+00]\n",
    "#  [  5.65001825e-03   1.83547742e-02   3.76482282e-01   3.25453318e-02\n",
    "#     0.00000000e+00   1.13346596e-02]\n",
    "#  [  0.00000000e+00   0.00000000e+00   4.80849077e-01   0.00000000e+00\n",
    "#     0.00000000e+00   0.00000000e+00]\n",
    "#  [  0.00000000e+00   9.01924448e-03   5.50933818e-01   0.00000000e+00\n",
    "#     0.00000000e+00   0.00000000e+00]\n",
    "#  [  0.00000000e+00   0.00000000e+00   4.65906981e-01   0.00000000e+00\n",
    "#     0.00000000e+00   0.00000000e+00]\n",
    "#  [  0.00000000e+00   1.14088541e-02   2.08627572e-02   5.17755511e-01\n",
    "#     5.81629743e-02   1.37868947e-02]\n",
    "#  [  0.00000000e+00   0.00000000e+00   0.00000000e+00   5.10463728e-01\n",
    "#     0.00000000e+00   0.00000000e+00]\n",
    "#  [  0.00000000e+00   5.60142127e-03   0.00000000e+00   4.22370287e-01\n",
    "#     0.00000000e+00   0.00000000e+00]\n",
    "#  [  0.00000000e+00   0.00000000e+00   0.00000000e+00   4.36741361e-01\n",
    "#     0.00000000e+00   0.00000000e+00]\n",
    "#  [  0.00000000e+00   0.00000000e+00   0.00000000e+00   4.98080756e-01\n",
    "#     0.00000000e+00   0.00000000e+00]\n",
    "#  [  9.88366761e-02   8.60100620e-02   3.90983687e-03   3.81008870e-01\n",
    "#     4.39421028e-04   5.22211692e-03]\n",
    "#  [  0.00000000e+00   0.00000000e+00   0.00000000e+00   5.72156883e-01\n",
    "#     0.00000000e+00   7.13627097e-03]\n",
    "#  [  1.31465075e-02   1.04860315e-02   0.00000000e+00   4.68895412e-01\n",
    "#     0.00000000e+00   1.16322294e-02]\n",
    "#  [  3.84539282e-03   0.00000000e+00   0.00000000e+00   5.75697492e-01\n",
    "#     0.00000000e+00   0.00000000e+00]\n",
    "#  [  2.25239129e-03   1.38747181e-03   0.00000000e+00   5.27933729e-01\n",
    "#     1.20310496e-02   1.49500333e-02]\n",
    "#  [  0.00000000e+00   4.07574718e-01   1.85689474e-03   0.00000000e+00\n",
    "#     2.96723410e-03   4.52365662e-04]\n",
    "#  [  1.53416989e-03   6.08212641e-01   5.22205076e-04   6.24838316e-03\n",
    "#     1.18489465e-03   4.40109910e-04]\n",
    "#  [  5.38804369e-03   2.65034312e-01   5.38434864e-04   1.86921587e-02\n",
    "#     6.38895284e-03   2.90132437e-03]\n",
    "#  [  0.00000000e+00   6.44957896e-01   0.00000000e+00   0.00000000e+00\n",
    "#     0.00000000e+00   0.00000000e+00]\n",
    "#  [  0.00000000e+00   6.08946624e-01   0.00000000e+00   0.00000000e+00\n",
    "#     0.00000000e+00   0.00000000e+00]\n",
    "#  [  0.00000000e+00   3.43707631e-01   0.00000000e+00   0.00000000e+00\n",
    "#     3.97946096e-03   0.00000000e+00]\n",
    "#  [  6.10492008e-03   3.15333353e-01   1.54859137e-02   0.00000000e+00\n",
    "#     5.06437136e-03   4.74379589e-03]\n",
    "#  [  6.47354568e-03   2.13342445e-01   9.49367504e-03   4.56970890e-02\n",
    "#     1.71980262e-02   9.52150420e-03]\n",
    "#  [  7.99125163e-03   4.67625618e-01   0.00000000e+00   2.43419796e-02\n",
    "#     0.00000000e+00   0.00000000e+00]\n",
    "#  [  0.00000000e+00   6.42861972e-01   0.00000000e+00   2.35849326e-03\n",
    "#     0.00000000e+00   0.00000000e+00]\n",
    "#  [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
    "#     4.77261103e-01   0.00000000e+00]\n",
    "#  [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
    "#     4.94440481e-01   0.00000000e+00]\n",
    "#  [  0.00000000e+00   2.99068686e-04   2.14451635e-03   0.00000000e+00\n",
    "#     3.81921702e-01   5.83830661e-03]\n",
    "#  [  0.00000000e+00   0.00000000e+00   0.00000000e+00   5.64675940e-03\n",
    "#     5.42444351e-01   0.00000000e+00]\n",
    "#  [  1.78052695e-03   7.84456293e-04   1.41608159e-02   4.59787746e-04\n",
    "#     4.24461218e-01   0.00000000e+00]\n",
    "#  [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
    "#     5.11582652e-01   0.00000000e+00]\n",
    "#  [  0.00000000e+00   0.00000000e+00   3.28335229e-03   0.00000000e+00\n",
    "#     3.73026564e-01   0.00000000e+00]\n",
    "#  [  0.00000000e+00   2.62097705e-04   3.61054985e-02   2.32322492e-04\n",
    "#     2.30597045e-01   0.00000000e+00]\n",
    "#  [  1.12514342e-02   2.12340752e-03   1.60950068e-02   1.02482072e-02\n",
    "#     3.25583605e-01   3.75915749e-02]\n",
    "#  [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
    "#     4.19115076e-01   3.57704275e-04]\n",
    "#  [  3.08364955e-01   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
    "#     0.00000000e+00   0.00000000e+00]\n",
    "#  [  3.68171423e-01   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
    "#     0.00000000e+00   0.00000000e+00]\n",
    "#  [  3.97942238e-01   2.81721472e-02   3.66962687e-03   1.70062680e-02\n",
    "#     1.96040569e-03   2.11664796e-02]\n",
    "#  [  3.75792129e-01   2.07534363e-03   0.00000000e+00   3.72145807e-02\n",
    "#     0.00000000e+00   5.85982994e-03]\n",
    "#  [  4.38025314e-01   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
    "#     0.00000000e+00   0.00000000e+00]\n",
    "#  [  4.57877996e-01   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
    "#     0.00000000e+00   0.00000000e+00]\n",
    "#  [  2.75475412e-01   4.46985945e-03   0.00000000e+00   5.29643478e-02\n",
    "#     0.00000000e+00   1.91015485e-02]\n",
    "#  [  4.45190987e-01   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
    "#     5.48904081e-03   0.00000000e+00]\n",
    "#  [  2.92738455e-01   1.33673503e-02   1.14247805e-02   1.05197510e-02\n",
    "#     1.87766704e-01   9.24051901e-03]\n",
    "#  [  3.78264000e-01   1.43979717e-02   0.00000000e+00   9.85216875e-02\n",
    "#     1.35951375e-02   0.00000000e+00]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 NMF features of the Wikipedia articles (prev example cont'd)\n",
    "Now you will explore the NMF features you created in the previous exercise. A solution to the previous exercise has been pre-loaded, so the array 'nmf_features' is available. Also available is a list 'titles' giving the title of each Wikipedia article.\n",
    "\n",
    "When investigating the features, notice that for both actors, the NMF feature 3 has by far the highest value. This means that both articles are reconstructed using mainly the 3rd NMF component. In the next video, you'll see why: NMF components represent topics (for instance, acting!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Create a pandas DataFrame: df\n",
    "df = pd.DataFrame(nmf_features, index=titles)\n",
    "\n",
    "# Print the row for 'Anne Hathaway'\n",
    "print(df.loc['Anne Hathaway'])\n",
    "\n",
    "# Print the row for 'Denzel Washington'\n",
    "\n",
    "# print(df.loc['Denzel Washington'])\n",
    "# 0    0.003845\n",
    "# 1    0.000000\n",
    "# 2    0.000000\n",
    "# 3    0.575711\n",
    "# 4    0.000000\n",
    "# 5    0.000000\n",
    "# Name: Anne Hathaway, dtype: float64\n",
    "# 0    0.000000\n",
    "# 1    0.005601\n",
    "# 2    0.000000\n",
    "# 3    0.422380\n",
    "# 4    0.000000\n",
    "# 5    0.000000\n",
    "# Name: Denzel Washington, dtype: float64\n",
    "\n",
    "# Notice that for both actors, the NMF feature 3 has by far the \n",
    "# highest value. This means that both articles are reconstructed \n",
    "# using mainly the 3rd NMF component.\n",
    "# You'll se why next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 NMF reconstructs samples\n",
    "In this exercise, you'll check your understanding of how NMF reconstructs samples from its components using the NMF feature values. Below are the components of an NMF model. If the NMF feature values of a sample are [2, 1], then which of the following is most likely to represent the original sample? A pen and paper will help here! You have to apply the same technique Ben used in the video to reconstruct the sample [0.1203 0.1764 0.3195 0.141].\n",
    "\n",
    "components of a NMF model\n",
    "[[ 1.   0.5  0. ]\n",
    " [ 0.2  0.1  2.1]]\n",
    "\n",
    "NMF feature values of a sample\n",
    "[2, 1]\n",
    "\n",
    "original sample = components * features\n",
    "1*2+0.2*1, .5*2+.1*1, 0*2+2.1*1\n",
    "2.4, 1.1, 2.1\n",
    "\n",
    "Answer: [2.2, 1.0, 2.0] is mostly likely to represent the original sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8 NMF learns interpretable parts\n",
    "Example: NMF learns interpretable parts\n",
    "- Word-frequency array articles (tf-idf)\n",
    "- 20,000 scientific articles (rows)\n",
    "- 800 words (columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying NMF to the articles\n",
    "print(articles.shape)\n",
    "# (20000, 800)\n",
    "from sklearn.decomposition import NMF\n",
    "nmf = NMF(n_components=10)\n",
    "nmf.fit(articles)\n",
    "print(nmf.component_.shape)\n",
    "# (10, 800)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.9 NMF components\n",
    "- For documents:\n",
    "    - NMF components represent topics\n",
    "    - NMF features combine topics into documents\n",
    "- For images, NMF components are parts of images\n",
    "\n",
    "Grayscale images\n",
    "- \"Grayscale\" image = no colors, only shades of gray\n",
    "- measure pixel brightness\n",
    "- Represent with value b/n 0 and 1 (0 is black)\n",
    "- Convert to 2D array\n",
    "- Example:\n",
    "    - 8x8 grayscale image of the moon in an array\n",
    "- Grayscale images as flat arrays\n",
    "    - enumerate the entries\n",
    "    - row-by-row\n",
    "    - from left to right\n",
    "    - from top to bottom\n",
    "\n",
    "Encoding a collection of images\n",
    "- collectino of images of the same size\n",
    "- encode as 2D array\n",
    "- each row corresponds to an image - as a flattened array\n",
    "- each column corresponds to a pixel\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.10 Visualizing samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample)\n",
    "bitmap = sample.reshape((2,3))\n",
    "# 2D array\n",
    "print(bitmap)\n",
    "\n",
    "# to recover the image, use the reshaped method of the sample\n",
    "# specify the dimensions of the original image as a tuple\n",
    "# Yields 2D array of pixel brightness\n",
    "print(bitmap)\n",
    "# To display the corresponding image\n",
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(bitmap, cmap='gray', interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.11 NMF learns topics of documents\n",
    "In the video, you learned when NMF is applied to documents, the components correspond to topics of documents, and the NMF features reconstruct the documents from the topics. Verify this for yourself for the NMF model that you built earlier using the Wikipedia articles. Previously, you saw that the 3rd NMF feature value was high for the articles about actors Anne Hathaway and Denzel Washington. In this exercise, identify the topic of the corresponding NMF component.\n",
    "\n",
    "The NMF model you built earlier is available as model, while words is a list of the words that label the columns of the word-frequency array.\n",
    "\n",
    "After you are done, take a moment to recognise the topic that the articles about Anne Hathaway and Denzel Washington have in common!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame: components_df\n",
    "components_df = pd.DataFrame(model.components_,columns=words)\n",
    "\n",
    "# Print the shape of the DataFrame\n",
    "print(components_df.shape)\n",
    "\n",
    "# Select row 3: component\n",
    "component = components_df.iloc[3]\n",
    "\n",
    "# Print result of nlargest\n",
    "# This gives the five words with the highest values for that component.\n",
    "print(component.nlargest())\n",
    "\n",
    "# (6, 13125)\n",
    "# film       0.627877\n",
    "# award      0.253131\n",
    "# starred    0.245284\n",
    "# role       0.211451\n",
    "# actress    0.186398\n",
    "# Name: 3, dtype: float64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.12 Explore the LED digits dataset\n",
    "In the following exercises, you'll use NMF to decompose grayscale images into their commonly occurring patterns. \n",
    "\n",
    "Firstly, explore the image dataset and see how it is encoded as an array. You are given 100 images as a 2D array 'samples', where each row represents a single 13x8 image. The images in your dataset are pictures of a LED digital display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pyplot\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Select the 0th row: digit\n",
    "digit = samples[0,:]\n",
    "\n",
    "# Print digit - it is a 1D array of 0s and 1s\n",
    "print(digit)\n",
    "\n",
    "# Reshape digit to a 13x8 2D array: bitmap\n",
    "bitmap = digit.reshape(13,8)\n",
    "\n",
    "# Print bitmap\n",
    "print(bitmap)\n",
    "\n",
    "# Use plt.imshow to display bitmap as an image\n",
    "plt.imshow(bitmap, cmap='gray', interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "# when you print bitmap, notice how the 1s show the digit 7\n",
    "[[ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
    " [ 0.  0.  1.  1.  1.  1.  0.  0.]\n",
    " [ 0.  0.  0.  0.  0.  0.  1.  0.]\n",
    " [ 0.  0.  0.  0.  0.  0.  1.  0.]\n",
    " [ 0.  0.  0.  0.  0.  0.  1.  0.]\n",
    " [ 0.  0.  0.  0.  0.  0.  1.  0.]\n",
    " [ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
    " [ 0.  0.  0.  0.  0.  0.  1.  0.]\n",
    " [ 0.  0.  0.  0.  0.  0.  1.  0.]\n",
    " [ 0.  0.  0.  0.  0.  0.  1.  0.]\n",
    " [ 0.  0.  0.  0.  0.  0.  1.  0.]\n",
    " [ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
    " [ 0.  0.  0.  0.  0.  0.  0.  0.]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.13 NMF learns the parts of images\n",
    "Now use what you've learned about NMF to decompose the digits dataset. You are again given the digit images as a 2D array 'samples'. This time, you are also provided with a function show_as_image() that displays the image encoded by any 1D array:\n",
    "\n",
    "def show_as_image(sample):\n",
    "\n",
    "    bitmap = sample.reshape((13, 8))\n",
    "    \n",
    "    plt.figure()\n",
    "    \n",
    "    plt.imshow(bitmap, cmap='gray', interpolation='nearest')\n",
    "    \n",
    "    plt.colorbar()\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "After you are done, take a moment to look through the plots and notice how NMF has expressed the digit as a sum of the components!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import NMF\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# Create an NMF model: model\n",
    "model = NMF(n_components=7)\n",
    "\n",
    "# Apply fit_transform to samples: features\n",
    "features = model.fit_transform(samples)\n",
    "\n",
    "# Call show_as_image on each component\n",
    "for component in model.components_:\n",
    "    show_as_image(component)\n",
    "\n",
    "# Assign the 0th row of features: digit_features\n",
    "digit_features = features[0]\n",
    "\n",
    "# Print digit_features\n",
    "print(digit_features)\n",
    "\n",
    "# [  4.76823559e-01   0.00000000e+00   0.00000000e+00   5.90605054e-01\n",
    "#    4.81559442e-01   0.00000000e+00   7.37551667e-16]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.14 PCA doesn't learn parts\n",
    "Unlike NMF, PCA doesn't learn the parts of things. Its components do not correspond to topics (in the case of documents) or to parts of images, when trained on images. Verify this for yourself by inspecting the components of a PCA model fit to the dataset of LED digit images from the previous exercise. The images are available as a 2D array 'samples'. \n",
    "\n",
    "Also available is a modified version of the show_as_image() function which colors a pixel red if the value is negative.\n",
    "\n",
    "After submitting the answer, notice that the components of PCA do not represent meaningful parts of images of LED digits!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat previous exercise with PCA components\n",
    "# Appreciate that PCA components doesn't have meaningful parts like NMF\n",
    "\n",
    "# Import PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create a PCA instance: model\n",
    "model = PCA(n_components=7)\n",
    "\n",
    "# Apply fit_transform to samples: features\n",
    "features = model.fit_transform(samples)\n",
    "\n",
    "# Call show_as_image on each component\n",
    "for component in model.components_:\n",
    "    show_as_image(component)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.15 Building recommender systems using NMF\n",
    "Example: Finding similar articles\n",
    "- Engineer at a large online newspaper\n",
    "- Task: recommend articles similar to article being read by customer\n",
    "- Similar articles should have similar topics\n",
    "- Strategy:\n",
    "    - Apply NMF to the word-frequency array\n",
    "    - NMF feature values describe the topics\n",
    "    - ...so similar documents have similar NMF feature values\n",
    "    - Compare NMF feature values?\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply NMF to 'articles', the word-frequency array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "nmf = NMF(n_components=6)\n",
    "nmf_features = nmf.fit_transform(articles)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Versions of articles\n",
    "- Different versions of the same document have same topic proportions\n",
    "- ...exact feature values may be different!\n",
    "    - ie. b/c one version uses many meaningless words\n",
    "        - Dog bites man\n",
    "        - vs. it seems that a dog has perhaps bitting a man\n",
    "- On a scatterplot of the NMF features, the weak and strong version all lie on a single line passing through the origin\n",
    "- in comparing documents, we need to compare these lines - cosine similarity\n",
    "- Cosine similarity\n",
    "    - = uses the angle b/n the 2 lines\n",
    "    - higher values means more similar\n",
    "    - max value = 1, when angle is 0 deg\n",
    "\n",
    "#### Calculating the cosine similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate cosine similarities\n",
    "from sklearn.preprocessing import normalize\n",
    "norm_features = normalize(nmf_features)\n",
    "current_article = norm_features[23,:] # if has index 23\n",
    "# compute cosine similarities\n",
    "similarities = norm_features.dot(current_article)\n",
    "print(similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataFrames and labels\n",
    "- label similarities with the article titles, using a DataFrame\n",
    "- Titles given as a list: titles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe of similarities\n",
    "\n",
    "import pandas as pd\n",
    "norm_features = normalize(nmf_features)\n",
    "df = pd.DataFrame(norm_features, index=titles)\n",
    "# select normalized features of the current article\n",
    "current_article = df.loc['Dog bites man']\n",
    "# calculate the cosine similarities\n",
    "simlarities = df.dot(current_article)\n",
    "# using .nlargest() find the articles with the highest cosine similarity\n",
    "print(similarities.nlargest())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.15.a Which articles are similar to 'Cristiano Ronaldo'?\n",
    "### Using cosine similarity\n",
    "\n",
    "In the video, you learned how to use NMF features and the cosine similarity to find similar articles. Apply this to your NMF model for popular Wikipedia articles, by finding the articles most similar to the article about the footballer Cristiano Ronaldo. The NMF features you obtained earlier are available as 'nmf_features', while 'titles' is a list of the article titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the necessary imports\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# Normalize the NMF features: norm_features\n",
    "norm_features = normalize(nmf_features)\n",
    "\n",
    "# Create a DataFrame: df\n",
    "df = pd.DataFrame(norm_features, index=titles)\n",
    "\n",
    "# Select the row corresponding to 'Cristiano Ronaldo': article\n",
    "article = df.loc['Cristiano Ronaldo']\n",
    "\n",
    "# Compute the dot products: similarities\n",
    "similarities = df.dot(article)\n",
    "\n",
    "# Display those with the largest cosine similarity\n",
    "print(similarities.nlargest())\n",
    "\n",
    "# Cristiano Ronaldo                1.000000\n",
    "# Franck Ribry                    0.999972\n",
    "# Radamel Falcao                   0.999942\n",
    "# Zlatan Ibrahimovi               0.999942\n",
    "# France national football team    0.999923\n",
    "# dtype: float64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.15.b Recommend musical artists part I\n",
    "In this exercise and the next, you'll use what you've learned about NMF to recommend popular music artists! You are given a sparse array artists whose rows correspond to 'artists' and whose column correspond to users. The entries give the number of times each artist was listened to by each user.\n",
    "\n",
    "In this exercise, build a pipeline and transform the array into normalized NMF features. The first step in the pipeline, 'MaxAbsScaler', transforms the data so that all users have the same influence on the model, regardless of how many different artists they've listened to. In the next exercise, you'll use the resulting normalized NMF features for recommendation!\n",
    "\n",
    "This data is part of a larger dataset available here. (note: link deleted since it was flagged as dangerous page)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part 1: compute the normalized NMF features\n",
    "\n",
    "# Perform the necessary imports\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import Normalizer, MaxAbsScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Create a MaxAbsScaler: scaler\n",
    "scaler = MaxAbsScaler()\n",
    "\n",
    "# Create an NMF model: nmf\n",
    "nmf = NMF(n_components=20)\n",
    "\n",
    "# Create a Normalizer: normalizer\n",
    "normalizer = Normalizer()\n",
    "\n",
    "# Create a pipeline: pipeline\n",
    "pipeline = make_pipeline(scaler, nmf, normalizer)\n",
    "\n",
    "# Apply fit_transform to artists: norm_features\n",
    "norm_features = pipeline.fit_transform(artists)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.16.a Recommend musical artists part II\n",
    "Suppose you were a big fan of Bruce Springsteen - which other musicial artists might you like? Use your NMF features from the previous exercise and the cosine similarity to find similar musical artists. A solution to the previous exercise has been run, so 'norm_features' is an array containing the normalized NMF features as rows. The names of the musical artists are available as the list 'artist_names'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part 2: use the normalized NMF features to recommend musical artists\n",
    "\n",
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame: df\n",
    "df = pd.DataFrame(norm_features, index=artist_names)\n",
    "\n",
    "# Select row of 'Bruce Springsteen': artist\n",
    "artist = df.loc['Bruce Springsteen']\n",
    "\n",
    "# Compute cosine similarities: similarities\n",
    "similarities = df.dot(artist)\n",
    "\n",
    "# Display those with highest cosine similarity\n",
    "print(similarities.nlargest())\n",
    "\n",
    "Bruce Springsteen    1.000000\n",
    "Neil Young           0.957043\n",
    "Van Morrison         0.875627\n",
    "Leonard Cohen        0.867585\n",
    "Bob Dylan            0.863132\n",
    "dtype: float64"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
